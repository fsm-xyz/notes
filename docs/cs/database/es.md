# es

## 基础

分片

副本

index

document

mappings text, keyword, 数值, geo, 对象, 数组, date, bool

每个字段单独建立倒排索引, 如果没有指定字段搜索，会默认全部索引的字段搜索或者默认字段

高基数: 单个字段的唯一值过多导致，分组，聚合数据量大

1. 避免不必要的keyword, 可以使用text
2. 拆分字段
3. 数据清洗和标准化


## 匹配

es如果不指定字段进行全文匹配的话 返回的结果能知道是匹配到具体的那一个字段嘛

highlight	结果显示、UI高亮	非常高	简单有效，既能定位字段又能直接用于前端展示	需要额外的开销来计算高亮
matched_queries	复杂布尔查询的匹配源追踪	中	精确控制，适用于复杂的搜索逻辑	配置稍复杂，需要为子查询命名
explain API	深度调试评分原理	低（禁用）	提供最详尽的内部分数计算细节


在 ElasticSearch（ES）面试中，关于分词和查询的面试问题通常是核心考察点。这些问题旨在评估你对 ES 工作原理的理解，以及你是否能有效地解决实际应用中的搜索问题。下面是一些常见的分词和查询面试问题，以及如何回答的思路。

### ES分词（Analysis）面试问题

#### 1. 什么是ES分词器（Analyzer），它由哪几部分组成？
**回答要点：**
* **定义：** 分词器是ES中用于处理文本的工具，它将原始文本分解成一个个独立的词项（term），并对这些词项进行标准化处理，以便于后续的索引和搜索。
* **组成部分：**
    * **Character Filters（字符过滤器）：** 在分词前处理原始文本。例如，可以移除HTML标签、转换特殊字符或执行自定义字符映射。
    * **Tokenizer（分词器）：** 核心部分，负责将文本切分成词项。例如，`standard`分词器按空格和标点符号切分，`icu_tokenizer`支持多语言。
    * **Token Filters（词项过滤器）：** 在分词后对生成的词项进行处理。例如，**小写转换（`lowercase`）**、**同义词处理（`synonym`）**、**停用词移除（`stop`）** 和**词干提取（`stemmer`）**。

#### 2. ES有哪些内置的分词器？它们的区别是什么？
**回答要点：**
* **`standard`：** 默认分词器，适用于大多数西方语言。它按单词边界切分，并进行小写转换和移除停用词。
* **`simple`：** 仅按非字母字符切分，并进行小写转换。例如，`Quick-Brown`会被分成`quick`和`brown`。
* **`whitespace`：** 仅按空格切分。它不会进行小写转换或其他处理。
* **`keyword`：** 不进行任何分词，将整个文本作为一个单一的词项。常用于处理不需要分词的字段，如ID、国家代码或SKU。
* **`ik_smart` 和 `ik_max_word`（中文分词插件）：** 这两个是常见的中文分词插件。
    * **`ik_smart`：** 智能分词，分词粒度较粗，会尽可能地将多个字组成一个词。
    * **`ik_max_word`：** 最大化分词，会尽可能地将文本切分成所有可能的词，粒度更细，通常用于更精确的搜索场景。

#### 3. 为什么分词对ES的搜索很重要？
**回答要点：**
* **索引阶段：** 搜索的本质是“反向索引”（inverted index）。在索引时，分词器将文档文本切分成一个个词项，并记录它们在文档中的位置。
* **搜索阶段：** 当用户输入查询时，相同的分词器也会对查询字符串进行分词。只有当查询的词项与索引中的词项匹配时，才能找到对应的文档。
* **匹配准确性：** 分词器直接决定了哪些词会被索引和搜索。选择合适的分词器能提高搜索的准确性和相关性。例如，使用中文分词器可以更好地处理中文句子，避免将一个词切分为多个字。

---

### ES查询（Query）面试问题

#### 1. 解释一下`term query`和`match query`的区别？
**回答要点：**
* **`term query`：**
    * 属于**精确匹配**。它不会对查询词进行分词，而是直接在**倒排索引**中查找完全相同的词项。
    * 通常用于查询**不分词的字段**，如ID、状态、标签等（`keyword`类型）。如果用于分词字段，则需要精确匹配到分词后的某个词项。
* **`match query`：**
    * 属于**全文检索**。它会对查询词进行分词，然后用分词后的词项去索引中进行匹配。
    * 适用于**分词字段**（`text`类型），能够处理自然语言搜索。

**举例说明：**
假设有一个`title`字段，内容是`The quick brown fox`，它使用了`standard`分词器。
* `match query`查询`quick brown`：会先将`quick brown`分词为`quick`和`brown`，然后查询包含这两个词项的文档，因此能找到该文档。
* `term query`查询`quick brown`：不会分词，直接去查找完全匹配`quick brown`的词项，通常找不到，因为索引中没有这个词项，只有`quick`和`brown`。

#### 2. ES中的`bool`查询是什么？它有哪几种子句？
**回答要点：**
* **定义：** `bool`查询是ES中最常用的复合查询，它允许你将多个查询子句组合在一起，形成复杂的查询逻辑。
* **四种主要子句：**
    * `must`：子句中的所有查询都必须匹配。相当于**AND**关系。评分会累加。
    * `should`：子句中的任一查询匹配即可。相当于**OR**关系。常用于提高相关性。
    * `filter`：子句中的所有查询都必须匹配。但与`must`不同的是，`filter`是**不计算相关性评分**的，因此性能更高，常用于过滤结果集。
    * `must_not`：子句中的查询必须不匹配。相当于**NOT**关系。

#### 3. `query`和`filter`有什么区别？在什么场景下使用它们？
**回答要点：**
* **`query`（查询上下文）：**
    * 用于**全文检索**，会计算每个匹配文档的**相关性评分**（`_score`），从而对结果进行排序。
    * 主要用于**`bool`查询的`must`和`should`子句**，来影响搜索结果的排序。
* **`filter`（过滤上下文）：**
    * 用于**结构化数据**的精确匹配或范围过滤，**不计算相关性评分**。
    * 由于不计算评分，ES可以对过滤结果进行**缓存**，因此性能更高。
    * 主要用于**`bool`查询的`filter`和`must_not`子句**，来缩小结果集，但不影响排序。

**总结：**
* 需要**相关性排序**（如，搜索商品名、文章内容）时，使用`query`。
* 需要**精确筛选**，且结果不影响排序（如，按价格范围、按分类、按品牌过滤）时，使用`filter`。

### 总结建议

在回答这些问题时，除了给出定义，最好能结合**实际例子**或**应用场景**来解释。这能体现你对知识的深刻理解，而不仅仅是死记硬背。例如，在回答分词问题时，可以举出中文分词的例子；在回答`query`和`filter`时，可以举一个电商网站的搜索场景。

在 ElasticSearch 中，“查询评分聚合”（Query Scoring Aggregation）这个概念，更准确地理解应该是 **“在查询结果集上进行聚合，并通过相关性评分（\_score）来进一步分析和展示数据”**。

这通常不意味着聚合本身会使用 `_score` 来计算桶（bucket）或指标（metric），而是指你的查询（`query`）负责筛选出相关的文档，而聚合（`aggregation`）则在这些经过筛选的文档集合上进行统计分析。

面试中，你可以从以下几个层次来回答这个问题，展示你对 ES 查询和聚合之间关系的深刻理解。

### 1\. 基本概念：查询决定聚合的范围

  * **核心思想：** `query` 和 `filter` 用于确定哪些文档会被纳入到聚合计算中。只有匹配了查询条件的文档，才会参与到后续的聚合运算。
  * **如何工作：** 当你执行一个包含 `query` 和 `aggregation` 的搜索时，ES 会先执行查询部分，找到所有相关的文档。然后，它在这些文档上执行聚合操作，生成统计结果。

**举例说明：**
假设你想找到所有关于“手机”的产品，并按品牌（`brand`）进行聚合计数。
在这个场景中，`match query` 负责找到所有包含“手机”的文档（这些文档会有一个相关性评分），而 `terms aggregation` 负责统计这些文档中不同品牌的数量。

```json
GET /products/_search
{
  "query": {
    "match": {
      "name": "手机"
    }
  },
  "aggs": {
    "brands": {
      "terms": {
        "field": "brand.keyword"
      }
    }
  }
}
```

### 2\. 高级应用：使用 `top_hits` 聚合

`top_hits` 聚合是“查询评分聚合”的最佳实践之一，因为它允许你获取每个聚合桶（bucket）中**得分最高的文档**。这完美地结合了查询的相关性评分和聚合的分类功能。

**应用场景：**

  * **电商网站：** 查找每个品类下最受欢迎（或评分最高）的3款商品。
  * **新闻网站：** 找出每个主题下最相关（或最新）的5篇文章。

**如何工作：**
`top_hits` 聚合可以放在任何桶聚合（如 `terms`、`range`）内部。它会在每个桶中，根据 `_score` （或自定义的排序）返回指定数量的文档。

**举例说明：**
假设你想按品牌（`brand`）对手机进行聚合，并且在每个品牌下，展示评分最高的2款手机。

```json
GET /products/_search
{
  "query": {
    "match": {
      "name": "手机"
    }
  },
  "size": 0,  // 不返回原始文档，只返回聚合结果
  "aggs": {
    "brands": {
      "terms": {
        "field": "brand.keyword"
      },
      "aggs": {
        "top_rated_products": {
          "top_hits": {
            "size": 2, // 在每个桶中返回2个文档
            "sort": [
              {
                "_score": { // 按相关性评分排序
                  "order": "desc"
                }
              }
            ]
          }
        }
      }
    }
  }
}
```

### 3\. 进阶探讨：利用 `script` 聚合 `_score`

虽然不常见，但在某些特殊情况下，你可以利用 `script` 聚合来对文档的 `_score` 本身进行统计，例如计算平均得分。但这通常不是一个推荐的常规做法，因为 `_score` 是动态计算的，且聚合的目的是统计数据，而不是计算相关性。

**举例说明（仅作概念展示）：**

```json
GET /products/_search
{
  "query": {
    "match": {
      "name": "手机"
    }
  },
  "size": 0,
  "aggs": {
    "average_score": {
      "avg": {
        "script": {
          "source": "_score"
        }
      }
    }
  }
}
```

### 面试总结

在面试中，你可以这样总结：

  * **核心：** 查询和过滤决定了聚合的文档集合。
  * **主流实践：** 使用 `top_hits` 聚合来展示每个聚合桶中最相关的文档，这是“查询评分聚合”最常见的应用场景。
  * **性能考量：** 了解 `query` 和 `filter` 的区别，在需要精确过滤时使用 `filter`，因为它可以被缓存，性能更高，且不计算评分。
  * **避免误区：** 不要误以为聚合本身会像查询一样计算 `_score`，而是要清楚地认识到聚合是在**查询筛选后的结果集上**进行的。



在 ElasticSearch 中，并没有像 SQL 那样直接的 `LIMIT` 和 `OFFSET` 关键字。ES 使用 `size` 和 `from` 参数来实现相同的功能。

### `size` 和 `from` 参数

  * **`size`**: 类似于 SQL 的 `LIMIT`，用于控制返回的文档数量。默认值为 10。
  * **`from`**: 类似于 SQL 的 `OFFSET`，用于指定从哪个位置开始返回文档。默认值为 0。

`from` 和 `size` 的组合就是我们常说的**分页**。例如：

  * `"from": 0, "size": 10`：获取第一页数据（第1-10条）。
  * `"from": 10, "size": 10`：获取第二页数据（第11-20条）。
  * `"from": 20, "size": 10`：获取第三页数据（第21-30条）。

<!-- end list -->

```json
GET /_search
{
  "query": {
    "match_all": {}
  },
  "from": 10,
  "size": 10
}
```

-----

### `size` + `from` 的深分页问题

虽然 `from` 和 `size` 简单易用，但它们有一个**致命的缺陷**，这就是**深分页问题（Deep Pagination Problem）**。

#### 原理

ES 的分页是**基于分片（Shard）** 实现的。当你请求 `from: 990, size: 10` 时：

1.  协调节点（Coordinating Node）会向每个分片发送请求。
2.  每个分片都会独立地获取并排序前 `from + size`（即 1000）条数据。
3.  每个分片将这 1000 条数据的 ID 和排序值发送给协调节点。
4.  协调节点收到所有分片的结果后，对所有数据（假设有5个分片，就是 5 \* 1000 = 5000 条）进行全局排序。
5.  最后，协调节点从全局排序后的结果中，跳过前 990 条，返回最后的 10 条数据。

随着 `from` 值越来越大，每个分片需要处理和发送的数据量也会线性增长，这会消耗大量的**内存、CPU 和网络带宽**，导致集群性能急剧下降，甚至可能崩溃。

-----

### 解决方案

为了解决深分页问题，ES 提供了两种更高效的方案。

#### 1\. `search_after`

`search_after` 是一种基于**游标（Cursor）** 的分页方法，它使用上一页的最后一个文档作为下一页的起点。

  * **原理**: `search_after` 接收一个数组作为参数，该数组包含上一页最后一个文档的排序值。ES 利用这些值，直接定位到下一页的开始位置，而无需遍历和排序之前的文档。
  * **优点**: 性能极高，没有深分页问题，非常适合**实时、高并发的滚动分页**场景。
  * **缺点**:
      * **不能跳页**：只能一页一页地往后翻，不能直接跳到任意一页。
      * **需要排序**：必须指定一个唯一的排序字段（如 `_score` + `_id` 或 `timestamp` + `_id`），以确保分页的稳定性和一致性。

<!-- end list -->

```json
GET /_search
{
  "size": 10,
  "sort": [
    {
      "timestamp": "asc"
    },
    {
      "_id": "asc"
    }
  ],
  "search_after": [1465401918459, "doc-id-xyz"],
  "query": {
    "match_all": {}
  }
}
```

#### 2\. `Scroll API`

`Scroll API` 旨在创建一个“快照”，专门用于**大规模数据导出**。

  * **原理**: `scroll` 查询会创建一个临时上下文（快照），并将查询结果分批次返回。每次请求都使用一个 `_scroll_id` 来获取下一批数据，直到没有更多数据为止。
  * **优点**: 专为高效导出大量数据设计，不受深分页问题影响。
  * **缺点**:
      * **不适合实时搜索**: 由于创建了快照，它无法反映数据实时变化。
      * **资源消耗**: 快照会占用集群资源，需要在 `scroll` 参数中指定过期时间。

### 总结和面试建议

  * **日常分页**: 小于10000条数据，使用 `from` 和 `size` 即可，简单方便。
  * **高并发实时分页**: 如果需要**无限滚动加载**或翻页深度很大，强烈推荐使用 `search_after`。这是目前最优的实时分页方案。
  * **大规模数据导出**: 如果是为了导出或批量处理数据，应该使用 `Scroll API`。

在面试中，当你被问到 ES 的分页时，不要只回答 `from` 和 `size`，一定要**深入分析深分页问题及其原理**，并提出 `search_after` 和 `Scroll API` 这两种解决方案，这会极大地加分。